在使用pytorch进行网络训练的时候，有时候不可避免的使用迁移学习，使用已经训练好的模型（如resnet、inception等），固定其已经训练好的网络层参数，然后进行finetune。 

以下代码是以resnet101为例进行finetune：

```
import torch
import torch.nn as nn
from torchvision import models

model=models.resnet101(pretrained=True)

#对于模型的每个权重，使其不进行反向传播，即固定参数
for param in model.parameters():
    param.requires_grad = False
#不固定最后一层，即全连接层fc
for param in model.fc.parameters():
    param.requires_grad = True
```

这个时候如果按常规训练模型的方法直接使用 optimizer 的话会报错：

```
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
-------
ValueError: optimizing a parameter that doesn't require gradients
```

这是因为optimizer的输入参数parameters必须都是可以修改、反向传播的，即 `requires_grad=True`

解决方法是optimizer中只输入需要反向传播的参数：

```
optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),lr=0.1)
```

如果想修改最后一层的话，可以这么修改：

```
class_num = 200
channel_in = model.fc.in_features
model.fc = nn.Linear(channel_in.class_num)
```

也可以删除最后一层：

```
new_model = nn.Sequential(*list(model.children())[:-1])
```

