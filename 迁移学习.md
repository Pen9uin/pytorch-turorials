在使用pytorch进行网络训练的时候，有时候不可避免的使用迁移学习，使用已经训练好的模型（如resnet、inception等），固定其已经训练好的网络层参数，然后进行finetune。 

以下代码是以resnet101为例进行finetune：

```
import torch
import torch.nn as nn
from torchvision import models

model=models.resnet101(pretrained=True)

#对于模型的每个权重，使其不进行反向传播，即固定参数
for param in model.parameters():
    param.requires_grad = False
#不固定最后一层，即全连接层fc
for param in model.fc.parameters():
    param.requires_grad = True
```

这个时候如果按常规训练模型的方法直接使用 optimizer 的话会报错：

```
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
-------
ValueError: optimizing a parameter that doesn't require gradients
```

这是因为optimizer的输入参数parameters必须都是可以修改、反向传播的，即 `requires_grad=True`

解决方法是optimizer中只输入需要反向传播的参数：

```
optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, model.parameters()),lr=0.1)
```

如果想修改最后一层的话，可以这么修改：

```
class_num = 200
channel_in = model.fc.in_features
model.fc = nn.Linear(channel_in.class_num)
```

也可以删除最后一层：

```
new_model = nn.Sequential(*list(model.children())[:-1])
```
#### 获取中间层的输出

```python
m = models.vgg11()
for k,_ in m._modules.items():
    print(k)
#features
#classifier

for k,layer in m._modules.items():
    print(k,layer)
"""
features Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU(inplace)
  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (4): ReLU(inplace)
  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU(inplace)
  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (9): ReLU(inplace)
  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (12): ReLU(inplace)
  (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (14): ReLU(inplace)
  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (17): ReLU(inplace)
  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (19): ReLU(inplace)
  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
classifier Sequential(
  (0): Linear(in_features=25088, out_features=4096, bias=True)
  (1): ReLU(inplace)
  (2): Dropout(p=0.5)
  (3): Linear(in_features=4096, out_features=4096, bias=True)
  (4): ReLU(inplace)
  (5): Dropout(p=0.5)
  (6): Linear(in_features=4096, out_features=1000, bias=True)
)
"""
for _,layer in m.features._modules.items():
    x = layer(x)
```
